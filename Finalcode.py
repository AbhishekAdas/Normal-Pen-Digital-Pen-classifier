# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eC7j_X60PnLINLLSyrKxrsFRMQF8hrv6
"""

import cv2
import numpy as np
import glob
import pickle
import keras
import PIL
#Library Used
from keras.utils import to_categorical
from sklearn import metrics
from keras.models import Sequential
import numpy as np
import cv2
from sklearn import svm
import pickle
import random
from scipy.ndimage import rotate
import random
import os
import tensorflow as tf
from sklearn.linear_model import LogisticRegression
from skimage.feature import hog
import gc
from keras.datasets import mnist
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras import backend as K
from keras.layers.normalization import BatchNormalization
import matplotlib.pyplot as plt
from keras.layers import Activation, LeakyReLU
from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support
import seaborn as sb
#name = "CNN_basic_1.0.0.1"
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping
import pandas as pd
from skimage import feature

#DIGITAL_PEN_FILE = r"E:\Machine learning\Hyundai mobis\datasets\datasets\All_digital_pens\*.jpg"
#PEN_FILE = r"E:\Machine learning\Hyundai mobis\datasets\datasets\Normal_pens\*.jpg"
PICKLE_IMAGE_FILE = "/content/imgs"
PICKLE_LABEL_FILE = "/content/labels"

class LocalBinaryPatterns:
    def __init__(self, numPoints, radius):
        # store the number of points and radius
        self.numPoints = numPoints
        self.radius = radius

    def describe(self, image, eps=1e-7):
        # compute the Local Binary Pattern representation
        # of the image, and then use the LBP representation
        # to build the histogram of patterns
        lbp = feature.local_binary_pattern(image, self.numPoints,
            self.radius, method="uniform")
        (hist, _) = np.histogram(lbp.ravel(),
            bins=np.arange(0, self.numPoints + 3),
            range=(0, self.numPoints + 2))

        # normalize the histogram
        hist = hist.astype("float")
        hist /= (hist.sum() + eps)

        # return the histogram of Local Binary Patterns
        return hist
    

class digital_normal_pen_class:
    def __init__(self):
        pass
    
    def load_data(self):
        digital_pen = []
        for i in glob.glob(DIGITAL_PEN_FILE):
            img = cv2.imread(i)
            img = cv2.resize(img,(128,128))
            digital_pen.append(img)
        digital_pen = np.array(digital_pen)
        label_digi = np.ones(len(glob.glob(DIGITAL_PEN_FILE)))
        #visualize digital pen image
        plt.figure()
        plt.imshow(digital_pen[0]) 
        plt.show() 
        
        normal_pen = []
        for i in glob.glob(PEN_FILE):    
            img = cv2.imread(i)
            img = cv2.resize(img,(128,128))
            normal_pen.append(img)
        normal_pen = np.array(normal_pen)
        label_normal = np.zeros(len(glob.glob(PEN_FILE)))
        #visualize normal pen image
        plt.figure()
        plt.imshow(normal_pen[0]) 
        plt.show() 
        
        total_imgs = np.concatenate((digital_pen, normal_pen))
        total_labels = np.concatenate((label_digi,label_normal))
        
#        #pickle data for later use
#        dbfile = open(PICKLE_IMAGE_FILE, 'wb')
#        pickle.dump(total_imgs, dbfile) 
#        dbfile.close()
#        #    
#        
#        dbfile = open(PICKLE_LABEL_FILE, 'wb')
#        pickle.dump(total_labels, dbfile) 
#        dbfile.close()
        return total_imgs,total_labels
        
        
    def augment_data(self,data,label):
        def flip(img):
            img = cv2.flip(img,1)
            return img
           
        def Gaussianblur(image):
            img = cv2.blur(image, ksize = (10,10))
            return img
        
        def sharpening(img):
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            img = cv2.filter2D(img, -1, kernel)
            return img
        
        def scaling(img):
            return cv2.resize(img, (128,128))  
        
        augmentations = [flip, Gaussianblur, sharpening, scaling]
            
        new_list_image = []
        new_list_label = []
        data = np.squeeze(np.array(data))
                
        for i in range(data.shape[0]):
            for j in range(len(augmentations)):
                image_to_be_appended = augmentations[j](data[i])
                new_list_image.append(image_to_be_appended)
                new_list_label.append(label[i])
            
            # Uncomment the below code if you wish to increase the number of images and take the normal image along with augmentations
            image_to_be_appended = data[i]
            new_list_image.append(image_to_be_appended)
            new_list_label.append(label[i])         
                
        images_augmented = np.array(new_list_image)
        labels_augmented = np.array(new_list_label)
        return images_augmented, labels_augmented
        
    def read_data(self):
      
        num_classes = 2
        dbfile = open(PICKLE_IMAGE_FILE, 'rb')
        xtr = pickle.load(dbfile)
        dbfile.close()

        dbfile = open(PICKLE_LABEL_FILE, 'rb')
        ytr = pickle.load(dbfile)
        dbfile.close()
        
        Xtrain = xtr
        ytrain = ytr
        print('Number of images before augmentation:  ',Xtrain.shape)
       
        Xtrain, ytrain = self.augment_data(Xtrain, ytrain)
        print('Number of images after augmentation:  ',type(Xtrain))
        Xtrain,Xtest,ytrain,ytest = train_test_split(Xtrain,ytrain,shuffle = True)
        #Train Test Image Shape
        
        print('Train data shape:   ',Xtrain.shape)
        print('Test data shape:   ',Xtest.shape)     
        return Xtrain,Xtest,ytrain,ytest

    def generate_new_features(self,data):
        desc = LocalBinaryPatterns(24, 8)
        hist = []
        for imag in data:
            gray = cv2.cvtColor(imag, cv2.COLOR_BGR2GRAY)
            hist.append(desc.describe(gray))
        hog_images = []
        hog_features = []
        for image in data:
            fd, hog_image = hog(image, orientations=9, multichannel=True, pixels_per_cell=(8, 8),cells_per_block=(2, 2), visualize=True)    
            hog_images.append(hog_image)
            hog_features.append(fd)
        return hist,hog_features
    
    def create_model(self,model_name):
        if  model_name=='SVM':
            model = svm.SVC(kernel = 'linear',C=10,max_iter = 10000)
        elif model_name=='Logistic':
            model = LogisticRegression(max_iter=1000)
        elif model_name== 'CNN':
            patience = 5
            krs = (3,3)
            input_shape = (128,128,3)
            num_classes = 2
            model = Sequential()    
            model.add(Conv2D(32, kernel_size=krs,input_shape=input_shape))
            
            model.add(MaxPooling2D(pool_size=(2, 2)))
            model.add(BatchNormalization())
            model.add(LeakyReLU(alpha=0.05))
            model.add(Dropout(0.3))
            
            '''model.add(Conv2D(64, krs))
            model.add(MaxPooling2D(pool_size=(2, 2)))
            model.add(BatchNormalization())
            model.add(LeakyReLU(alpha=0.05))
            model.add(Dropout(0.3))
            
            model.add(Conv2D(128, krs))
            model.add(MaxPooling2D(pool_size=(2, 2)))
            model.add(BatchNormalization())
            model.add(Activation('relu'))
            model.add(Dropout(0.3))'''
            
            model.add(Flatten())
            
            # model.add(Dense(1024))
            # model.add(LeakyReLU(alpha=0.05))
            # model.add(Dropout(0.5))
            
            # model.add(Dense(512))
            #model.add(LeakyReLU(alpha=0.05))
            #model.add(Dropout(0.3))
            
            model.add(Dense(128))
            model.add(LeakyReLU(alpha=0.05))
            model.add(Dropout(0.3))
            
            model.add(Dense(num_classes, activation='softmax'))
            model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(lr=0.00001), metrics=['accuracy'])
        return model
    
    def train_model(self,model_name,Xtrain,Xtest,ytrain,ytest):
        if (model_name=='SVM'):
            lbp_features_hist,hog_features = self.generate_new_features(Xtrain)
            hog_features = np.array(hog_features)
            model = self.create_model(model_name)
            model.fit(hog_features,ytrain)
            print('yes')
        elif (model_name=='Logistic'):
            lbp_features_hist,hog_features = self.generate_new_features(Xtrain)
            model = self.create_model(model_name)
            hog_features = np.array(hog_features)
            model.fit(hog_features,ytrain)
        elif (model_name== 'CNN'):
            num_classes = 2
            ytrain = keras.utils.to_categorical(ytrain, num_classes)
            ytest = keras.utils.to_categorical(ytest, num_classes)
            model = self.create_model(model_name)
            Xtrain = Xtrain/255
            Xtest = Xtest/255
            history = model.fit(Xtrain, ytrain, batch_size=10, epochs=30, verbose=1, validation_data=(Xtest, ytest))
        
            #plot for train and validation loss/accuracy
            plt.plot(history.history['accuracy'])
            plt.plot(history.history['val_accuracy'])
            plt.title('model accuracy')
            plt.ylabel('accuracy')
            plt.xlabel('epoch')
            plt.legend(['train', 'test'], loc='upper left')
            plt.show()
            # summarize history for loss
            plt.plot(history.history['loss'])
            plt.plot(history.history['val_loss'])
            plt.title('model loss')
            plt.ylabel('loss')
            plt.xlabel('epoch')
            plt.legend(['train', 'test'], loc='upper left')
            plt.show()
        return model
    
    def predict_model(self,model,Xtest,ytest,model_name):
      if model_name=='SVM' or model_name == 'Logistic':
        lbp_features_hist,hog_features = self.generate_new_features(Xtest)
        hog_features = np.array(hog_features)
        y_pred = model.predict(hog_features)
      elif model_name=='CNN':
        y_pred = model.predict(Xtest)
        num_classes = 2
        ytest = keras.utils.to_categorical(ytest, num_classes)
      print(metrics.classification_report(ytest,y_pred))

classifier = digital_normal_pen_class()
#images, labels = classifier.load_data()
Xtrain,Xtest,ytrain,ytest = classifier.read_data()
model = classifier.train_model('CNN',Xtrain,Xtest,ytrain,ytest)
classifier.predict_model(model,Xtest,ytest,'CNN')



